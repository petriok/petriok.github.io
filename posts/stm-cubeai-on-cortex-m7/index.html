<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>STM CubeAI on Cortex M7 | IoT On The Edge</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="generator" content="Hugo 0.74.3" />
    
    
      <META NAME="ROBOTS" CONTENT="INDEX, FOLLOW">
    

    
    
      <link href="/dist/css/app.4fc0b62e4b82c997bb0041217cd6b979.css" rel="stylesheet">
    

    

    
      

    

    
    
    <meta property="og:title" content="STM CubeAI on Cortex M7" />
<meta property="og:description" content="Introduction This post is a brief performance analysis for running machine learning inference (prediction) on a Cortex-M7 microcontroller. I will concentrate here on the micro-optimizations by inspecting the code at assembly level.
The trained model is borrowed from the benchmarks done by Stupid Projects and adapted to my setup.
Test setup The MCU under test is an STM32H7 with 2MBytes of flash and 1MByte RAM, running at 480MHz. For an embedded microcontroller, this is truly a high-end device." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://petriok.github.io/posts/stm-cubeai-on-cortex-m7/" />
<meta property="article:published_time" content="2020-08-25T22:01:35+03:00" />
<meta property="article:modified_time" content="2020-08-25T22:01:35+03:00" />
<meta itemprop="name" content="STM CubeAI on Cortex M7">
<meta itemprop="description" content="Introduction This post is a brief performance analysis for running machine learning inference (prediction) on a Cortex-M7 microcontroller. I will concentrate here on the micro-optimizations by inspecting the code at assembly level.
The trained model is borrowed from the benchmarks done by Stupid Projects and adapted to my setup.
Test setup The MCU under test is an STM32H7 with 2MBytes of flash and 1MByte RAM, running at 480MHz. For an embedded microcontroller, this is truly a high-end device.">
<meta itemprop="datePublished" content="2020-08-25T22:01:35+03:00" />
<meta itemprop="dateModified" content="2020-08-25T22:01:35+03:00" />
<meta itemprop="wordCount" content="1134">



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="STM CubeAI on Cortex M7"/>
<meta name="twitter:description" content="Introduction This post is a brief performance analysis for running machine learning inference (prediction) on a Cortex-M7 microcontroller. I will concentrate here on the micro-optimizations by inspecting the code at assembly level.
The trained model is borrowed from the benchmarks done by Stupid Projects and adapted to my setup.
Test setup The MCU under test is an STM32H7 with 2MBytes of flash and 1MByte RAM, running at 480MHz. For an embedded microcontroller, this is truly a high-end device."/>

      
<script type="application/javascript">
var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
var doNotTrack = (dnt == "1" || dnt == "yes");
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	if (window.sessionStorage) {
		var GA_SESSION_STORAGE_KEY = 'ga:clientId';
		ga('create', 'UA-176411587-1', {
	    'storage': 'none',
	    'clientId': sessionStorage.getItem(GA_SESSION_STORAGE_KEY)
	   });
	   ga(function(tracker) {
	    sessionStorage.setItem(GA_SESSION_STORAGE_KEY, tracker.get('clientId'));
	   });
   }
	ga('set', 'anonymizeIp', true);
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

    
	
  </head>

  <body class="ma0 avenir bg-near-white production">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        IoT On The Edge
      
    </a>
    <div class="flex-l items-center">
      

      
      







<a href="https://www.linkedin.com/in/petri-oksanen-b7b269b2/" target="_blank" class="link-transition linkedin link dib z-999 pt3 pt0-l mr1" title="LinkedIn link" rel="noopener" aria-label="follow on LinkedIn——Opens in a new window">
  <svg  height="32px"  style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
  <path d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>









    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked">
          
        POSTS
      </aside>
      




  <div id="sharing" class="mt3">

    
    <a href="https://www.facebook.com/sharer.php?u=https://petriok.github.io/posts/stm-cubeai-on-cortex-m7/" class="facebook no-underline" aria-label="share on Facebook">
      <svg height="32px"  style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M28.765,50.32h6.744V33.998h4.499l0.596-5.624h-5.095  l0.007-2.816c0-1.466,0.14-2.253,2.244-2.253h2.812V17.68h-4.5c-5.405,0-7.307,2.729-7.307,7.317v3.377h-3.369v5.625h3.369V50.32z   M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;"/></svg>

    </a>

    
    
    <a href="https://twitter.com/share?url=https://petriok.github.io/posts/stm-cubeai-on-cortex-m7/&amp;text=STM%20CubeAI%20on%20Cortex%20M7" class="twitter no-underline" aria-label="share on Twitter">
      <svg height="32px"  style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167,22.283c-2.619,0.953-4.274,3.411-4.086,6.101  l0.063,1.038l-1.048-0.127c-3.813-0.487-7.145-2.139-9.974-4.915l-1.383-1.377l-0.356,1.017c-0.754,2.267-0.272,4.661,1.299,6.271  c0.838,0.89,0.649,1.017-0.796,0.487c-0.503-0.169-0.943-0.296-0.985-0.233c-0.146,0.149,0.356,2.076,0.754,2.839  c0.545,1.06,1.655,2.097,2.871,2.712l1.027,0.487l-1.215,0.021c-1.173,0-1.215,0.021-1.089,0.467  c0.419,1.377,2.074,2.839,3.918,3.475l1.299,0.444l-1.131,0.678c-1.676,0.976-3.646,1.526-5.616,1.568  C19.775,43.256,19,43.341,19,43.405c0,0.211,2.557,1.397,4.044,1.864c4.463,1.377,9.765,0.783,13.746-1.568  c2.829-1.673,5.657-5,6.978-8.221c0.713-1.716,1.425-4.851,1.425-6.354c0-0.975,0.063-1.102,1.236-2.267  c0.692-0.678,1.341-1.419,1.467-1.631c0.21-0.403,0.188-0.403-0.88-0.043c-1.781,0.636-2.033,0.551-1.152-0.402  c0.649-0.678,1.425-1.907,1.425-2.267c0-0.063-0.314,0.042-0.671,0.233c-0.377,0.212-1.215,0.53-1.844,0.72l-1.131,0.361l-1.027-0.7  c-0.566-0.381-1.361-0.805-1.781-0.932C39.766,21.902,38.131,21.944,37.167,22.283z M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4  s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/></svg>

    </a>

    
    <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://petriok.github.io/posts/stm-cubeai-on-cortex-m7/&amp;title=STM%20CubeAI%20on%20Cortex%20M7" class="linkedin no-underline" aria-label="share on LinkedIn">
      <svg  height="32px"  style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
  <path d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>

    </a>
  </div>


      <h1 class="f1 athelas mt3 mb1">STM CubeAI on Cortex M7</h1>
      
      
      <time class="f6 mv4 dib tracked" datetime="2020-08-25T22:01:35+03:00">August 25, 2020</time>

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links nested-img mid-gray pr4-l w-two-thirds-l"><h1 id="introduction">Introduction</h1>
<p>This post is a brief performance analysis for running machine learning inference (prediction) on a Cortex-M7 microcontroller. I will concentrate here on the micro-optimizations by inspecting the code at assembly level.</p>
<p>The trained model is borrowed from the benchmarks done by <a href="https://www.stupid-projects.com/machine-learning-on-embedded-part-1/">Stupid Projects</a> and adapted to my setup.</p>
<h2 id="test-setup">Test setup</h2>
<p>The MCU under test is an STM32H7 with 2MBytes of flash and 1MByte RAM, running at 480MHz. For an embedded microcontroller, this is truly a high-end device.</p>
<p>STM32CubeMX version 6.0.0 with STM&rsquo;s X-CUBE-AI version 5.1.2 were used to generate the base for the project. As far as I understand, the <a href="https://www.st.com/content/ccc/resource/legal/legal_agreement/license_agreement/group0/eb/8e/f9/c1/9e/64/49/95/SLA0048/files/SLA0048.txt/jcr:content/translations/en.SLA0048.txt">license</a> for X-CUBE-AI doesn&rsquo;t prohibit publishing the analysis what I&rsquo;m about to show here.</p>
<p>GNU Arm Embedded Toolchain version 9-2019-q4-major with -O3 optimization level is used in these tests.</p>
<p>Both the instruction and data caches are enabled in the test.</p>
<p>On the test firmware I have <a href="https://www.freertos.org/">FreeRTOS</a> running, but I disable the interrupts before running the inference to even eliminate any context switches from the benchmark.</p>
<h2 id="machine-learning-model">Machine learning model</h2>
<p>The model used in this example is a copy of the one used in <a href="https://www.stupid-projects.com/machine-learning-on-embedded-part-4/">Dimitri&rsquo;s</a> tests. It is a model for classifying a handwritten digit. The MNIST handwritten digits database is used to get the training data. I wanted first to use this exact same model for my test to see that I get comparable results and everything is working as expected.
Running it through STMCubeMX CubeAI plugin I get the memory consumption described as follows:
<a href="/images/stm32_cubeai/stm_cubeai_memory_usage.png"><img src="/images/stm32_cubeai/stm_cubeai_memory_usage.png" alt="CubeAI MNIST model memory consumption"></a></p>
<p>In this test I use a pre-made digit <a href="https://github.com/dimtass/stm32f746-x-cube-ai-mnist/blob/master/source/src/inc/digit.h">found from here</a> as an input, though for the performance analysis it doesn&rsquo;t matter whether it&rsquo;s meaningful or random data.</p>
<h2 id="inference-without-memory-optimizations">Inference without memory optimizations</h2>
<p>The first test to get started, the X-CUBE-AI inference takes about 36ms on this test setup.
<a href="/images/stm32_cubeai/ai_run_trace_stat_no_dtcm_no_itcm.png"><img src="/images/stm32_cubeai/ai_run_trace_stat_no_dtcm_no_itcm.png" alt="STM CubeAI on Cortex-M7, no TCM used"></a></p>
<p>During this test, the CubeAI library code is running and the model weights are stored in the flash. The task which is running the inference has a stack which is placed in SRAM. The heap is also placed in SRAM. The activations are also placed in SRAM.</p>
<h2 id="inference-with-some-memory-optimizations">Inference with some memory optimizations</h2>
<p>This one has the RAM part (activations, that 32.23kB stated by CubeAI) of the inference moved to the tightly coupled data memory DTCM so the memory reads and writes are single cycle.</p>
<p><a href="/images/stm32_cubeai/ai_run_trace_stat_dtcm_used.png"><img src="/images/stm32_cubeai/ai_run_trace_stat_dtcm_used.png" alt="STM CubeAI on Cortex-M7, DTCM used"></a></p>
<h2 id="inference-with-more-memory-optimizations">Inference with more memory optimizations</h2>
<p>The microcontroller under test has a total of 192kB tightly coupled memory which is divided between the instruction (ITCM, 64kB) and data (DTCM, 128kB) regions. The memory needed for the weights (that 257.54kB stated by CubeAI) is too large to put to tightly coupled memory, but the runtime fits to DTCM. In this test whole the runtime library (NetworkRuntime512_CM7_GCC.a in my case) is placed to DTCM.</p>
<p><a href="/images/stm32_cubeai/ai_run_trace_stat_dtcm_and_itcm.png"><img src="/images/stm32_cubeai/ai_run_trace_stat_dtcm_and_itcm.png" alt="STM CubeAI on Cortex-M7, DTCM and ITCM used"></a></p>
<p>So what&rsquo;s going on here, the results of moving the runtime library to ITCM is basically the same as when it is running from flash? Well, I stated in the test setup section that the instruction cache is enabled. I&rsquo;ll do one more test having the instruction cache disabled to see the effect.</p>
<h2 id="inference-with-instruction-cache-disabled">Inference with instruction cache disabled</h2>
<p>Here&rsquo;s a test where the AI runtime is running from flash, instruction cache disabled. I&rsquo;ll let the activations alone and let them stay in ITCM.</p>
<p><a href="/images/stm32_cubeai/ai_run_trace_stat_no_dtcm_no_icache.png"><img src="/images/stm32_cubeai/ai_run_trace_stat_no_dtcm_no_icache.png" alt="STM CubeAI on Cortex-M7, ITCM used, icache disabled"></a></p>
<p>That looks sensible. If the runtime is running from flash and the instruction cache is enabled (why wouldn&rsquo;t it be), a more realistic benchmark would be to mess with the cache, invalidate it and so on. Also running the inference interrupts disabled would not be very realistic, unless such a 36ms <em>stop the world</em> period is acceptable for the application.</p>
<h2 id="inference-with-data-cache-disabled">Inference with data cache disabled</h2>
<p>Here&rsquo;s a test where the AI runtime is running from flash, data cache disabled, instruction cache enabled.</p>
<p><a href="/images/stm32_cubeai/ai_run_trace_stat_no_dtcm_no_dcache.png"><img src="/images/stm32_cubeai/ai_run_trace_stat_no_dtcm_no_dcache.png" alt="STM CubeAI on Cortex-M7, ITCM used, icache disabled"></a></p>
<h2 id="inference-with-caches-disabled-no-tightly-coupled-memory">Inference with caches disabled, no tightly coupled memory</h2>
<p>Ok now I&rsquo;m running backwards, but just for fun as a worst case test I&rsquo;ll disable both the data and instruction caches and don&rsquo;t use any TCM.</p>
<p><a href="/images/stm32_cubeai/ai_run_trace_stat_no_caches_no_tcm.png"><img src="/images/stm32_cubeai/ai_run_trace_stat_no_caches_no_tcm.png" alt="STM CubeAI on Cortex-M7, no caches, no TCM"></a></p>
<p>Makes sense, move along, nothing to see here.</p>
<h2 id="final-test">Final test</h2>
<p>This is the last test. I looked at the <em>data cache disabled</em> numbers and started wondering how the performance got so bad. In the test setup section I stated that the AI runtime FreeRTOS task stack was in the SRAM. Finally, the test with everything that is possible in the TCM and caches enabled.</p>
<p><a href="/images/stm32_cubeai/ai_run_trace_stat_both_caches_all_in_tcm.png"><img src="/images/stm32_cubeai/ai_run_trace_stat_both_caches_all_in_tcm.png" alt="STM CubeAI on Cortex-M7, no caches, no TCM"></a></p>
<p>Not a very surprising result, it&rsquo;s similar where I started from. So what&rsquo;s to learn from here? - Make sure that the caches are turned on, or use the TCM if possible. On this test I let it run more cycles, but as the inference is running interrupts disabled there shouldn&rsquo;t be any variation in the timing. If there was, something would be terribly wrong..</p>
<h2 id="deeper-dive">Deeper dive</h2>
<p>Let&rsquo;s have closer look at what is actually happening when running the inference. This test runs the inference, waits for a second and does another. The <em>ERROR</em> in the chart is basically when the CPU is running in the IDLE task. TraceON/TraceOFF breakpoints are around the aiRun function.</p>
<p><a href="/images/stm32_cubeai/ai_run_chart_symbol_out.png"><img src="/images/stm32_cubeai/ai_run_chart_symbol_out.png" alt="STM CubeAI on Cortex-M7 large trace"></a></p>
<p>Let&rsquo;s dive in deeper to the action.</p>
<p><a href="/images/stm32_cubeai/ai_run_chart_symbol_single_inference.png"><img src="/images/stm32_cubeai/ai_run_chart_symbol_single_inference.png" alt="STM CubeAI on Cortex-M7 single inference"></a></p>
<p>There you have it, this is machine learning.</p>
<p>From this view it seems that the dominant functions are forward_conv2d_nl_pool and &hellip; memcpy! Ok, it&rsquo;s not the full story, but certainly an interesting place for optimization. Let&rsquo;s see more closer.</p>
<p><a href="/images/stm32_cubeai/ai_run_conv2d_memcpy.png"><img src="/images/stm32_cubeai/ai_run_conv2d_memcpy.png" alt="STM CubeAI on Cortex-M7 single inference, zoomed in"></a></p>
<p>From a wider view it is noticeable that there are two parts of the 2D convolution. In the first part (from the green cursor to the red cursor, about 8ms) the memcpy domination is not too bad.</p>
<p><a href="/images/stm32_cubeai/ai_run_chart_symbol_wide_single_inference.png"><img src="/images/stm32_cubeai/ai_run_chart_symbol_wide_single_inference.png" alt="STM CubeAI on Cortex-M7 single inference"></a></p>
<p>The &lsquo;second part&rsquo; of the convolution (from the red cursor to the blue cursor, about 23ms), memcpy plays a bigger role:</p>
<p><a href="/images/stm32_cubeai/ai_run_conv2d_memcpy_2.png"><img src="/images/stm32_cubeai/ai_run_conv2d_memcpy_2.png" alt="STM CubeAI on Cortex-M7 single inference, zoomed in"></a></p>
<p>And how is the memcpy implemented? It is doing a byte by byte copy:</p>
<p><a href="/images/stm32_cubeai/ai_run_memcpy_libc_nano.png"><img src="/images/stm32_cubeai/ai_run_memcpy_libc_nano.png" alt="Byte by byte memcpy assembly"></a></p>
<p>I&rsquo;m linking the test firmware with <code>-specs=nano.specs</code>, replacing this memcpy with an optimized version is for another post. Though if I was creating the CubeAI library I would keep the memory copying in my own hands.</p>
<p>And what about the convolution? This is a snippet of what is going on in the loop:</p>
<p><a href="/images/stm32_cubeai/ai_run_conv2d.png"><img src="/images/stm32_cubeai/ai_run_conv2d.png" alt="2D convolution assembly"></a></p>
<p>What can you determine from this snippet? Well, at least you can see that the FPU registers are well utilized, almost all of them. For Cortex-M7 this means that the instruction dual issue is utilized at least in the loading of registers. The repeated fused multiply accumulate instructions with the same destination register raises questions if there is a better way to arrange the instructions.</p>
<h2 id="conclusion">Conclusion</h2>
<p>For STM CubeAI, at least for the 2d convolution, there seems to be a quick win in the optimization field if the copying of memory from one place to another is done more efficiently (not byte by byte).</p>
<p>Next up will be having a deeper look at Tensorflow TFlite. It will be interesting to compare the tight convolution loop with CubeAI (I admit, I already had a look).</p>
<p>-Petri</p>
<ul class="pa0">
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="https://petriok.github.io/" >
    &copy;  IoT On The Edge 2021 
  </a>
    <div>







<a href="https://www.linkedin.com/in/petri-oksanen-b7b269b2/" target="_blank" class="link-transition linkedin link dib z-999 pt3 pt0-l mr1" title="LinkedIn link" rel="noopener" aria-label="follow on LinkedIn——Opens in a new window">
  <svg  height="32px"  style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
  <path d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>








</div>
  </div>
</footer>

    

  <script src="/dist/js/app.3fc0f988d21662902933.js"></script>


  </body>
</html>
